AWSTemplateFormatVersion: '2010-09-09'
Description: CloudFormation template to create an AWS Bedrock Agent resource, Lambda layer, and Lambda functions.

Parameters:
  FoundationModel:
    Type: String
    Default: 'anthropic.claude-3-haiku-20240307-v1:0'
  Alias:
    Type: String
    Default: '{ENTER ALIAS}'

Resources:
  # Lambda to create the layer from a custom resource
  LayerCreatorLambda:
    Type: 'AWS::Lambda::Function'
    Properties:
      FunctionName: !Sub 'LayerCreatorLambda-${Alias}'
      Handler: index.lambda_handler
      Role: !GetAtt LayerCreatorLambdaExecutionRole.Arn
      Runtime: python3.12
      Timeout: 120
      MemorySize: 1024
      Code:
        ZipFile: |
          import boto3
          import urllib.request
          import os
          import cfnresponse

          lambda_client = boto3.client('lambda')

          def download_from_github(url, filename):
              print(f"Downloading {url} to /tmp/{filename}")
              urllib.request.urlretrieve(url, f"/tmp/{filename}")
              print(f"Downloaded to /tmp/{filename}")

          def create_layer(layer_name, description, filename):
              with open(f"/tmp/{filename}", "rb") as f:
                  layer_content = f.read()

              response = lambda_client.publish_layer_version(
                  LayerName=layer_name,
                  Description=description,
                  Content={'ZipFile': layer_content},
                  CompatibleRuntimes=['python3.8', 'python3.9', 'python3.12']
              )
              return response['LayerVersionArn']

          def lambda_handler(event, context):
              try:
                  github_url = event['ResourceProperties']['github_url']
                  layer_name = event['ResourceProperties']['layer_name']
                  description = event['ResourceProperties']['description']
                  filename = event['ResourceProperties']['filename']

                  # Download the file from GitHub
                  download_from_github(github_url, filename)

                  # Create Lambda Layer
                  layer_arn = create_layer(layer_name, description, filename)

                  # Return Layer ARN to CloudFormation
                  response_data = {'LayerArn': layer_arn}
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, response_data)
              except Exception as e:
                  print(f"Error creating layer: {e}")
                  cfnresponse.send(event, context, cfnresponse.FAILED, {}, str(e))

  LayerCreatorLambdaExecutionRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      Policies:
        - PolicyName: 'LambdaLayerCreationPolicy'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'lambda:PublishLayerVersion'
                  - 's3:GetObject'
                Resource: '*'

  BedrockAgentExecutionRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: bedrock.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonBedrockFullAccess
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole

  CustomLayerResource:
    Type: 'Custom::LambdaLayer'
    Properties:
      ServiceToken: !GetAtt LayerCreatorLambda.Arn
      github_url: "https://github.com/build-on-aws/bedrock-agents-webscraper/raw/refs/heads/main/lambda-layer/layer-python-requests-googlesearch-beatifulsoup.zip"
      layer_name: "lambda-layer-python-requests-googlesearch-beautifulsoup"
      description: "Lambda layer for requests, googlesearch, and BeautifulSoup"
      filename: "layer-python-requests-googlesearch-beautifulsoup.zip"

  WebscrapeLambdaExecutionRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - !Ref CloudWatchLogsPolicy
      Policies:
        - PolicyName: 'SQSSendMessagePolicy'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'sqs:SendMessage'
                Resource:
                  - !GetAtt WebscrapeLambdaDLQ.Arn  # Add permission for the Webscrape Lambda DLQ
                  - !GetAtt InternetSearchLambdaDLQ.Arn  # Add permission for the InternetSearch Lambda DLQ


  CloudWatchLogsPolicy:
    Type: 'AWS::IAM::ManagedPolicy'
    Properties:
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Action:
              - 'logs:CreateLogGroup'
              - 'logs:CreateLogStream'
              - 'logs:PutLogEvents'
            Resource: !Sub "arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/*:*"

  WebscrapeLambdaDLQ:
    Type: 'AWS::SQS::Queue'
    Properties:
      QueueName: !Sub "WebscrapeLambdaDLQ-${AWS::AccountId}-${AWS::Region}"

  WebscrapeLambda:
    Type: 'AWS::Lambda::Function'
    DependsOn: CustomLayerResource
    Properties:
      FunctionName: !Sub 'WebscrapeLambda-${Alias}'
      Handler: index.lambda_handler
      Role: !GetAtt WebscrapeLambdaExecutionRole.Arn
      Runtime: python3.12
      MemorySize: 1024
      Timeout: 120
      DeadLetterConfig:
        TargetArn: !GetAtt WebscrapeLambdaDLQ.Arn
      Environment:
        Variables:
          S3Output: !Sub "s3://sl-webscrape-output-${Alias}-${AWS::AccountId}-${AWS::Region}/"
      Layers:
        - !GetAtt CustomLayerResource.LayerArn
      Code:
        ZipFile: |
          import urllib.request
          import os
          import shutil
          import json
          from bs4 import BeautifulSoup

          def get_page_content(url):
              try:
                  req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
                  with urllib.request.urlopen(req) as response:
                      if response.geturl() != url:  # Check if there were any redirects
                          print(f"Redirect detected for {url}")
                          return None
                      elif response:
                          return response.read().decode('utf-8')
                      else:
                          raise Exception("No response from the server.")
              except Exception as e:
                  print(f"Error while fetching content from {url}: {e}")
                  return None

          def empty_tmp_folder():
              try:
                  for filename in os.listdir('/tmp'):
                      file_path = os.path.join('/tmp', filename)
                      if os.path.isfile(file_path) or os.path.islink(file_path):
                          os.unlink(file_path)
                      elif os.path.isdir(file_path):
                          shutil.rmtree(file_path)
                  print("Temporary folder emptied.")
                  return "Temporary folder emptied."
              except Exception as e:
                  print(f"Error while emptying /tmp folder: {e}")
                  return None

          def save_to_tmp(filename, content):
              try:
                  if content is not None:
                      print(content)
                      with open(f'/tmp/{filename}', 'w') as file:
                          file.write(content)
                      print(f"Saved {filename} to /tmp")
                      return f"Saved {filename} to /tmp"
                  else:
                      raise Exception("No content to save.")
              except Exception as e:
                  print(f"Error while saving {filename} to /tmp: {e}")
                  return None

          def check_tmp_for_data(query):
              try:
                  data = []
                  for filename in os.listdir('/tmp'):
                      if query in filename:
                          with open(f'/tmp/{filename}', 'r') as file:
                              data.append(file.read())
                  print(f"Found {len(data)} file(s) in /tmp for query {query}")
                  return data if data else None
              except Exception as e:
                  print(f"Error while checking /tmp for query {query}: {e}")
                  return None

          def handle_search(event):
              parameters = event.get('parameters', [])
              input_url = next((param['value'] for param in parameters if param['name'] == 'inputURL'), '')

              if not input_url:
                  return {"error": "No URL provided"}

              if not input_url.startswith(('http://', 'https://')):
                  input_url = 'http://' + input_url

              tmp_data = check_tmp_for_data(input_url)
              if tmp_data:
                  return {"results": tmp_data}

              empty_tmp_result = empty_tmp_folder()
              if empty_tmp_result is None:
                  return {"error": "Failed to empty /tmp folder"}

              content = get_page_content(input_url)
              if content is None:
                  return {"error": "Failed to retrieve content"}

              cleaned_content = parse_html_content(content)

              filename = input_url.split('//')[-1].replace('/', '_') + '.txt'
              save_result = save_to_tmp(filename, cleaned_content)

              if save_result is None:
                  return {"error": "Failed to save to /tmp"}

              return {"results": {'url': input_url, 'content': cleaned_content}}

          def parse_html_content(html_content):
              soup = BeautifulSoup(html_content, 'html.parser')
              for script_or_style in soup(["script", "style"]):
                  script_or_style.decompose()
              text = soup.get_text()
              lines = (line.strip() for line in text.splitlines())
              chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
              cleaned_text = '\n'.join(chunk for chunk in chunks if chunk)

              max_size = 25000
              if len(cleaned_text) > max_size:
                  cleaned_text = cleaned_text[:max_size]

              return cleaned_text


          def lambda_handler(event, context):
              response_code = 200
              action_group = event['actionGroup']
              api_path = event['apiPath']

              print("THE EVENT: ", event)

              if api_path == '/search':
                  result = handle_search(event)
              else:
                  response_code = 404
                  result = f"Unrecognized api path: {action_group}::{api_path}"

              response_body = {
                  'application/json': {
                      'body': result
                  }
              }

              action_response = {
                  'actionGroup': event['actionGroup'],
                  'apiPath': event['apiPath'],
                  'httpMethod': event['httpMethod'],
                  'httpStatusCode': response_code,
                  'responseBody': response_body
              }

              api_response = {'messageVersion': '1.0', 'response': action_response}
              print("action_response: ", action_response)
              print("response_body: ", response_body)
              return api_response

  InternetSearchLambdaDLQ:
    Type: 'AWS::SQS::Queue'
    Properties:
      QueueName: !Sub "InternetSearchLambdaDLQ-${Alias}-${AWS::Region}"

  InternetSearchLambda:
    Type: 'AWS::Lambda::Function'
    DependsOn: CustomLayerResource
    Properties:
      FunctionName: !Sub 'InternetSearchLambda-${Alias}'
      Handler: index.lambda_handler
      Role: !GetAtt WebscrapeLambdaExecutionRole.Arn
      Runtime: python3.12
      MemorySize: 1024
      EphemeralStorage:
        Size: 4048
      Timeout: 120
      DeadLetterConfig:
        TargetArn: !GetAtt InternetSearchLambdaDLQ.Arn
      Environment:
        Variables:
          S3Output: !Sub "s3://sl-internet-search-output-${Alias}-${AWS::AccountId}-${AWS::Region}/"
      Layers:
        - !GetAtt CustomLayerResource.LayerArn
      Code:
        ZipFile: |
          import json
          import urllib.request
          import os
          import shutil
          from googlesearch import search
          from bs4 import BeautifulSoup

          def get_page_content(url):
              try:
                  req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
                  with urllib.request.urlopen(req) as response:
                      if response:
                          soup = BeautifulSoup(response.read().decode('utf-8'), 'html.parser')
                          for script_or_style in soup(["script", "style"]):
                              script_or_style.decompose()
                          text = soup.get_text()
                          lines = (line.strip() for line in text.splitlines())
                          chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                          cleaned_text = '\n'.join(chunk for chunk in chunks if chunk)
                          return cleaned_text
                      else:
                          raise Exception("No response from the server.")
              except Exception as e:
                  print(f"Error while fetching and cleaning content from {url}: {e}")
                  return None

          def empty_tmp_directory():
              try:
                  folder = '/tmp'
                  for filename in os.listdir(folder):
                      file_path = os.path.join(folder, filename)
                      try:
                          if os.path.isfile(file_path) or os.path.islink(file_path):
                              os.unlink(file_path)
                          elif os.path.isdir(file_path):
                              shutil.rmtree(file_path)
                      except Exception as e:
                          print(f"Failed to delete {file_path}. Reason: {e}")
                  print("Temporary directory emptied.")
              except Exception as e:
                  print(f"Error while emptying /tmp directory: {e}")

          def save_content_to_tmp(content, filename):
              try:
                  if content is not None:
                      with open(f'/tmp/{filename}', 'w', encoding='utf-8') as file:
                          file.write(content)
                      print(f"Saved {filename} to /tmp")
                      return f"Saved {filename} to /tmp"
                  else:
                      raise Exception("No content to save.")
              except Exception as e:
                  print(f"Error while saving {filename} to /tmp: {e}")

          def search_google(query):
              try:
                  search_results = []
                  for j in search(query, sleep_interval=5, num_results=10):
                      search_results.append(j)
                  return search_results
              except Exception as e:
                  print(f"Error during Google search: {e}")
                  return []

          def handle_search(event):
              input_text = event.get('inputText', '')

              print("Emptying temporary directory...")
              empty_tmp_directory()

              print("Performing Google search...")
              urls_to_scrape = search_google(input_text)

              aggregated_content = ""
              results = []
              for url in urls_to_scrape:
                  print("URLs Used: ", url)
                  content = get_page_content(url)
                  if content:
                      print("CONTENT: ", content)
                      filename = url.split('//')[-1].replace('/', '_') + '.txt'
                      aggregated_content += f"URL: {url}\n\n{content}\n\n{'='*100}\n\n"
                      results.append({'url': url, 'status': 'Content aggregated'})
                  else:
                      results.append({'url': url, 'error': 'Failed to fetch content'})

              aggregated_filename = f"aggregated_{input_text.replace(' ', '_')}.txt"
              print("Saving aggregated content to /tmp...")
              save_result = save_content_to_tmp(aggregated_content, aggregated_filename)
              if save_result:
                  results.append({'aggregated_file': aggregated_filename, 'tmp_save_result': save_result})
              else:
                  results.append({'aggregated_file': aggregated_filename, 'error': 'Failed to save aggregated content to /tmp'})

              return {"results": results}

          def lambda_handler(event, context):
              print("THE EVENT: ", event)

              response_code = 200
              if event.get('apiPath') == '/search':
                  result = handle_search(event)
              else:
                  response_code = 404
                  result = {"error": "Unrecognized api path"}

              response_body = {
                  'application/json': {
                      'body': json.dumps(result)
                  }
              }

              action_response = {
                  'actionGroup': event['actionGroup'],
                  'apiPath': event['apiPath'],
                  'httpMethod': event['httpMethod'],
                  'httpStatusCode': response_code,
                  'responseBody': response_body
              }

              api_response = {'messageVersion': '1.0', 'response': action_response}
              print("RESPONSE: ", action_response)

              return api_response

  LambdaInvokePermission:
    Type: 'AWS::Lambda::Permission'
    DependsOn: WebscrapeLambda
    Properties:
      FunctionName: !GetAtt WebscrapeLambda.Arn
      Action: 'lambda:InvokeFunction'
      Principal: 'bedrock.amazonaws.com'
      SourceArn: !Sub 'arn:aws:bedrock:${AWS::Region}:${AWS::AccountId}:agent/*'
  LambdaInvokePermissionForWebscrape:
    Type: 'AWS::Lambda::Permission'
    Properties:
      FunctionName: !GetAtt WebscrapeLambda.Arn
      Action: 'lambda:InvokeFunction'
      Principal: 'bedrock.amazonaws.com'
      SourceArn: !Sub 'arn:aws:bedrock:${AWS::Region}:${AWS::AccountId}:agent/*'
  LambdaInvokePermissionForInternetSearch:
    Type: 'AWS::Lambda::Permission'
    Properties:
      FunctionName: !GetAtt InternetSearchLambda.Arn
      Action: 'lambda:InvokeFunction'
      Principal: 'bedrock.amazonaws.com'
      SourceArn: !Sub 'arn:aws:bedrock:${AWS::Region}:${AWS::AccountId}:agent/*'

  BedrockAgent:
    Type: "AWS::Bedrock::Agent"
    DependsOn: LambdaInvokePermission
    Properties:
      AgentName: !Sub 'WebscrapeAgent-${Alias}'
      AgentResourceRoleArn: !GetAtt BedrockAgentExecutionRole.Arn
      AutoPrepare: 'True'
      FoundationModel: !Ref FoundationModel
      Instruction: |
        You are a research analyst that webscrapes websites, and searches the internet to provide information based on a {question}. You provide concise answers in a friendly manner.
      Description: "Uses a web URL to search the internet"
      IdleSessionTTLInSeconds: 900
      ActionGroups:
        - ActionGroupName: "webscrape"
          Description: "This action group is used to search the internet based on a web URL."
          ActionGroupExecutor:
            Lambda: !Sub 'arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:function:WebscrapeLambda-${AWS::AccountId}'
          ApiSchema:
            Payload: |
              {
                "openapi": "3.0.0",
                "info": {
                  "title": "Webscrape API", 
                  "description": "An API that will take in a URL, then scrape and store the content from the URL in an S3 bucket.",
                  "version": "1.0.0"
                },
                "paths": {
                  "/search": {
                    "post": {
                      "description": "content scraping endpoint",
                      "parameters": [
                        {
                          "name": "inputURL",
                          "in": "query",
                          "description": "URL to scrape content from",
                          "required": true,
                          "schema": {
                            "type": "string"
                          }
                        }
                      ],
                      "responses": {
                        "200": {
                          "description": "Successful response",
                          "content": {
                            "application/json": {
                              "schema": {
                                "type": "object",
                                "properties": {
                                  "upload_result": {
                                    "type": "string",
                                    "description": "Result of uploading content to S3"
                                  }
                                }
                              }
                            }
                          }
                        }
                      }
                    }
                  }
                }
              }
        - ActionGroupName: "internet-search"
          Description: "This action group is used to search the internet."
          ActionGroupExecutor:
            Lambda: !Sub 'arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:function:InternetSearchLambda-${Alias}'
          ApiSchema:
            Payload: |
              {
                "openapi": "3.0.0",
                "info": {
                  "title": "Internet Search API", 
                  "description": "An API that will take in user input, then will conduct an internet search that matches the inquiry as close as possible",
                  "version": "1.0.0"
                },
                "paths": {
                  "/search": {
                    "post": {
                      "description": "Internet search endpoint",
                      "parameters": [
                        {
                          "name": "query",
                          "in": "query",
                          "description": "Search query text",
                          "required": true,
                          "schema": {
                            "type": "string"
                          }
                        }
                      ],
                      "requestBody": {
                        "description": "Additional internet serach parameters",
                        "required": true,
                        "content": {
                          "application/json": {
                            "schema": {
                              "type": "object",
                              "properties": {
                                "depth": {
                                  "type": "integer",
                                  "description": "Maximum search depth"
                                }
                              }
                            }
                          }
                        }
                      },
                      "responses": {
                        "200": {
                          "description": "Successful response",
                          "content": {
                            "application/json": {
                              "schema": {
                                "type": "array",
                                "items": {
                                  "type": "string"
                                }
                              }
                            }
                          }
                        }
                      }
                    }
                  }
                }
              }


  BedrockAgentAlias:
    Type: 'AWS::Bedrock::AgentAlias'
    DependsOn: BedrockAgent
    Properties:
      AgentAliasName: !Sub 'Alias-1'
      AgentId: !GetAtt BedrockAgent.AgentId

Outputs:
  BedrockAgentName:
    Description: 'Name of the Bedrock Agent created'
    Value: !Ref BedrockAgent
  WebscrapeLambdaArn:
    Description: 'ARN of the Webscrape Lambda function'
    Value: !GetAtt WebscrapeLambda.Arn
  InternSearchLambdaArn:
    Description: 'ARN of the InternetSearch Lambda function'
    Value: !GetAtt InternetSearchLambda.Arn
